<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">
    <link rel="stylesheet" href="node_modules/bootstrap/dist/css/bootstrap.min.css">

    <link rel="stylesheet" href="kujundus.css">
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-light bg-light">
        <img class="logo" src="images/logo.png" alt="logo">
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item active">
              <a class="nav-link" href="kodutoo_index.html">Home </a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="blog.html">Blogs</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="sale.html">Products for sale </a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="kontaktid.html">Contact</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="sisselogimine.html">Sign in</a>
              </li>
          </ul>
        </div>
      </nav>
    <div>
<p id="esimene" ><h4>Our progress on generative AI in health</h4>Last year at Google Health’s Check Up event, we introduced Med-PaLM 2, our large language model (LLM) fine-tuned for healthcare. Since introducing that research, the model has become available to a set of global customer and partner organizations that are building solutions for a range of uses — including streamlining nurse handoffs and supporting clinicians’ documentation. At the end of last year, we introduced MedLM, a family of foundation models for healthcare built on Med-PaLM 2, and made it more broadly available through Google Cloud’s Vertex AI platform.

    Since then, our work on generative AI for healthcare has progressed — from the new ways we’re training our health AI models to our latest research on applying AI to the healthcare industry.
    
    New modalities in models for healthcare
    Medicine is a multimodal discipline; it’s made up of different types of information stored across formats — like radiology images, lab results, genomics data, environmental context and more. To get a fuller understanding of a person’s health, we need to build technology that understands all of this information.
    
    We’re bringing new capabilities to our models with the hope of making generative AI more helpful to healthcare organizations and people’s health. We just introduced MedLM for Chest X-ray, which has the potential to help transform radiology workflows by helping with the classification of chest X-rays for a variety of use cases. We’re starting with Chest X-rays because they are critical in detecting lung and heart conditions. MedLM for Chest X-ray is now available to trusted testers in an experimental preview on Google Cloud.
    
    Research on fine-tuning our models for the medical domain
    Approximately 30% of the world’s data volume is being generated by the healthcare industry - and is growing at 36% annually. This includes large quantities of text, images, audio, and video. And further, important information about patients' histories is often buried deep in a medical record, making it difficult to find relevant information quickly.
    
    For these reasons, we’re researching how a version of the Gemini model, fine-tuned for the medical domain, can unlock new capabilities for advanced reasoning, understanding a high volume of context, and processing multiple modalities. Our latest research resulted in state-of-the-art performance on the benchmark for the U.S. Medical Licensing Exam (USMLE)-style questions at 91.1%, and on a video dataset called MedVidQA.
    
    And because our Gemini models are multimodal, we were able to apply this fine-tuned model to other clinical benchmarks — including answering questions about chest X-ray images and genomics information. We’re also seeing promising results from our fine-tuned models on complex tasks such as report generation for 2D images like X-rays, as well as 3D images like brain CT scans – representing a step-change in our medical AI capabilities. While this work is still in the research phase, there’s potential for generative AI in radiology to bring assistive capabilities to health organizations.</p>

    </div>

    <div><p id="teine"><h4>How AI supports early disease detection in India</h4>AI supported early detection for high-mortality diseases
        More than 10 million people a year become ill with tuberculosis (TB), and more than 1.3 million die from the disease worldwide annually, with particularly high rates in South Asia and Sub-Saharan Africa. TB can be treated and cured, but delays in treatment allow the disease to spread through the community and develop to the point of being fatal in patients.
        
        Chest X-rays are a common method of screening for the disease, but there are not enough trained radiologists to interpret images at scale. Further, TB is usually diagnosed with tests that are often not available in rural settings or areas where there are few hospitals and healthcare centers. To help, we’ve developed an AI system to interpret chest X-ray scans for early signs of TB.
        
        Lung cancer and breast cancer also require specialized screening and medical expertise to interpret scans. In India, lung cancer is one of the leading causes of cancer death. There is no nationalized screening program for high-risk individuals to try to catch the disease early, when treatment is most effective. Lung cancer can also be identified incidentally if a patient happens to get a scan for some other reason. However, radiologists need to be trained to identify these incidental nodules and have time to look for them.
        
        AI can help on both fronts, making screening more broadly accessible and providing an additional check to identify incidental nodules and follow-up as needed.
        
        India also has more than three times the death rate from breast cancer, compared to the U.S. There is a shortage of radiologists trained to identify breast cancer and screening is not widely accessible. We’re hopeful that AI can assist radiologists in interpreting mammograms to scale screening in India.</p></div>
<div><P id="kolmas"><h4>3 ways we are building equity into our health work</h4>
    A group of health equity researchers, social scientists, clinicians, bioethicists, statisticians, and AI researchers came together across Google to develop a framework for building AI that avoids creating and reinforcing unfair bias.
    
    This framework, which is called HEAL (Health Equity Assessment of Machine Learning performance), is designed to assess the likelihood that AI technology will perform equitably and to prevent AI models from being deployed that might make disparities worse — especially for groups that experience poorer health outcomes on average. The four-step process includes:
    
    Determining factors associated with health inequities and defining AI performance metrics.
    Identifying and quantifying pre-existing health outcome disparities.
    Measuring the performance of the AI tool for each subpopulation.
    Assessing the likelihood that the AI tool prioritizes performance with respect to health disparities.
    Already, we’ve used this framework to test a dermatology AI model. The results showed that while this model performed equitably across race, ethnicity and sex subgroups, there were improvements we could make to perform better for older age groups. The framework found that when it came to evaluating cancerous conditions, like melanoma, the model performed equitably across age groups, but for non-cancer conditions, like eczema, it did not perform as well across the 70 and older age group.
    
    We’ll continue to apply the framework to healthcare AI models in the future, and we’ll evolve and refine the framework in the process.</P></div>
<div><p id="neljas"><h4>How we are using AI for reliable flood forecasting at a global scale</h4>In a paper published today in Nature, we share how AI can help scale flood forecasting and bring help to parts of the world that are most impacted by climate change. We found that AI helped us to provide more accurate information on riverine floods up to 7 days in advance. This allowed us to provide flood forecasting in 80 countries in areas where 460 million people live. Where possible, we also provide forecasts in Google Search and Google Maps and via Android notifications.

    The paper — described in more detail in our Research blog — demonstrates how AI-based global hydrologic technologies built by Google Research can significantly improve flood forecasting relative to the current state-of-the-art. This is even true for countries where reliable flood-related data is scarce, making it possible to expand flood forecasting on a global scale. Early warning systems can significantly help reduce fatalities, and having more lead time is extremely helpful for communities. With these technologies we extended, on average, the reliability of currently-available global nowcasts from zero to five days, and we were able to use AI-based forecasting to improve forecasts in regions in Africa and Asia to be similar to what are currently available in Europe.
    
    Today, this information can be used by people, communities, governments and aid organizations to take anticipatory action to help protect vulnerable populations. Getting here hasn’t been easy, especially in regions where data is scarce and the impact of flooding is disproportionately large. Today, as we publish our latest paper, we thought we’d look back at some of the moments that shaped our journey in using AI to accurately forecast riverine floods:
    
    Our first pilot in India taught us a valuable lesson
    Our research work began with an initial pilot in India’s Patna region. Bihar, where Patna is located, is one of India’s most flood-prone states where a large part of the population lives under the recurring threat of devastating floods. Working with local government officials and using local real-time data, we created flood forecasts which we incorporated into Google Public Alerts in 2018.AI helps provide more accurate riverine flood information up to 7 days in advance. Flood forecasting is now available in 80 countries, covering 460 million people. AI-based forecasting improves flood predictions in regions with scarce data. Flood Hub platform provides free real-time forecasts to vulnerable communities.</p></div>
<div><p id="viies"><h4>Generation Sensation: New Generative AI and RTX Tools Boost Content Creation</h4>NVIDIA Omniverse Audio2Face for iClone 8 uses AI to produce expressive facial animations solely from audio input. In addition to generating natural lip-sync animations for multilingual dialogue, the latest standalone release supports multilingual lip-sync and singing animations, as well as full-spectrum editing with slider controls and a keyframe editor.Creators are getting a generative AI boost with tools announced at NVIDIA GTC, a global AI conference bringing together the brightest minds in AI content creation and accelerated computing.

    Adobe Substance 3D Stager and Sampler via Adobe Firefly, the OBS 30.1 YouTube HDR Beta and NVIDIA Omniverse Audio2Face for iClone 8 will also receive sizable upgrades</p></div>
    
<div><p id="kuues"> <h4>AI’s New Frontier: From Daydreams to Digital Deeds</h4>Imagine a world where you can whisper your digital wishes into your device, and poof, it happens.
   
   That world may be coming sooner than you think. But if you’re worried about AI doing your thinking for you, you might be waiting for a while.
   
   In a fireside chat Wednesday at NVIDIA GTC, the global AI conference, Kanjun Qiu, CEO of Imbue, and Bryan Catanzaro, vice president of applied deep learning research at NVIDIA, challenged many of the clichés that have long dominated conversations about AI.
   
   Launched in October 2022, Imbue made headlines with its Series B fundraiser last year, raising over $200 million at a $1 billion valuation.
   
   Bridging the Gap Between ‘Idea and Execution’
   
   The discussion highlighted not only Imbue’s approach toward building practical AI agents able to automate menial, unrewarding work, but also painted a vivid picture of what the next chapter in AI innovation might hold.
   
   “Our lives are full of so much friction … every single person’s vision can come to life,” Qiu said. “The barrier between idea and execution can be much smaller.”
   
   Catanzaro’s reflections on the practical difficulties of using AI for simple tasks, such as his own challenges trying to get his digital assistant to help him find his next meeting, underscored the current limitations in human-AI interaction.
   
   It turns out that figuring out where and when to go to a meeting, while easy for a human assistant, isn’t easy to automate.</p>

     
  </div>

</body>
</html>